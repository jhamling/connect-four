# â™Ÿï¸ Connect Four AI League & Tournament System

A **high-performance Connect Four AI framework** built for **experimentation, benchmarking, and research**.  
This project evaluates *dozens to hundreds* of AI agents under statistically grounded tournament conditions, with a clear separation between **strategic strength** and **computational efficiency**.

> This is not just a game â€” itâ€™s an **AI evaluation system**.

---

## ğŸš€ Highlights

- âš™ï¸ Fast, headless simulation engine
- ğŸ¤– Dozens of AI architectures with parameterized variants
- ğŸ“‰ Auto-pruning league tournament (scales to hundreds of agents)
- ğŸ“Š Statistically robust scoring (Wilson confidence bounds)
- âš–ï¸ Explicit strength vs. speed tradeoff analysis
- ğŸ“¦ Reproducible runs with deterministic seeding
- ğŸŒ Designed for future web visualization

---

## ğŸ® Game Engine

- Full **Connect Four ruleset**
- Headless game loop for maximum throughput
- Deterministic RNG seeding for reproducibility
- Opening randomization to avoid trivial or solved openings

---

## ğŸ¤– AI Agents

Implemented agent families:

- `Random`, `WeightedRandom`
- `Greedy`
- `TacticalGreedy`
- `Heuristic`
- `Minimax`
- `Expectiminimax`
- `Beam Search`
- `Beam Search v2` (shaping + move ordering)
- `Monte Carlo Tree Search (MCTS)`

Each agent supports **parameterized variants**:
- Search depth
- Temperature
- Time limits
- Exploration constants
- Heuristic weights

â¡ï¸ A single league run can easily include **hundreds of unique agents**.

---

## ğŸ† League Tournament System  
### Auto-Pruning Tournament

Instead of an infeasible full round-robin, agents compete in **staged tournaments**:

1. Large initial agent pool  
2. Randomized pairings per stage  
3. Statistical evaluation after each stage  
4. Automatic pruning of weaker agents  
5. Final round-robin among top performers  

âœ… Keeps runtime tractable  
âœ… Preserves competitive accuracy  
âœ… Avoids early noise dominating results  

---

## ğŸ“ Scoring & Metrics

The system **explicitly separates strength from speed**.

---

### ğŸ’ª Strength (Primary Metric)

Computed using a **Wilson Lower Confidence Bound (LCB)** over points-per-game:

Strength = WilsonLCB(PPG, z)


**Why Wilson LCB?**
- Penalizes lucky short runs
- Rewards consistency
- Statistically conservative
- Resistant to variance

---

### âš¡ Efficiency (Strength Ã— Speed)

To avoid rewarding only trivial fast agents:

Efficiency =
Strength Ã— max(
speed_min_factor,
1 / (1 + speed_alpha Ã— sqrt(ms_per_move / ms_target))
)


**Key properties**
- Fast agents are rewarded
- Slow but strong agents remain competitive
- Prevents â€œGreedy always winsâ€ artifacts
- Makes compute-efficiency visible

---

### ğŸ“Š Additional Tracked Stats

- Wins / Draws / Losses
- Games played
- Average milliseconds per move
- Node expansions
- Search depth
- Pareto frontier (strength vs. speed)

---

## ğŸ§ª CLI Usage

Run the project:

```bash
python -m connect4

Select:

Run AI League (auto-prune tournament)

Youâ€™ll be prompted for:

    Games per pairing

    Pairings per team per stage

    Minimum games before pruning

    Keep fraction

    Final survivor count

    Wilson Z value

    Speed penalty parameters

    Parallel worker count

âœ”ï¸ Sensible defaults are provided for quick testing.
ğŸ§  Recommended Final Run Settings

(Research-grade / conclusive data)
Setting	Value
Games per pairing	4
Pairings per team per stage	12
Min games before prune	48
Keep fraction	0.50
Final keep	50
Max stages	5â€“6
Wilson Z	1.96
ms_target	50
speed_alpha	0.35
speed_min_factor	0.75
Workers	6
Batch size	16

â±ï¸ Expected runtime: hours, not minutes.
ğŸ“¤ Output

At the end of a league run:

    Full ranking by Strength

    Full ranking by Efficiency

    Pareto frontier (speed vs. strength)

    CSV export of all agent statistics

Example:

Wrote CSV: league_results_YYYYMMDD_HHMMSS.csv

ğŸ—‚ï¸ Project Structure

connect4/
â”œâ”€â”€ ai/        # Agent implementations
â”œâ”€â”€ core/      # Board + rules
â”œâ”€â”€ game/      # Game state
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ league.py        # Tournament engine
â”‚   â”œâ”€â”€ league_main.py   # CLI entry
â”œâ”€â”€ ui/        # Terminal UI
â””â”€â”€ main.py

ğŸ¯ Design Goals

    Fair AI evaluation

    Statistical rigor

    Transparent speed vs. strength tradeoffs

    Scalable experimentation

    Web-ready architecture

ğŸ”® Future Roadmap

Planned extensions:

    ğŸŒ Web UI (agent selection, live matches)

    ğŸ“Š Interactive charts (Pareto plots, Elo-like curves)

    ğŸ¥ Game replays

    ğŸ”Œ API backend (FastAPI)

    ğŸ§  Self-play training hooks

ğŸ¤” Why This Is Interesting

This system answers deeper questions than â€œwho wins?â€:

    Which agents scale well?

    Which strategies are compute-efficient?

    Where is the Pareto frontier of intelligence vs. speed?

â¡ï¸ Itâ€™s a benchmarking framework, not just a game.
ğŸ‘¤ Author

Jacob Hamling
University of Iowa
Informatics Â· Analytics Â· Finance

Focus areas:

    AI systems

    Evaluation methodology

    Applied decision modeling


If you want, I can also:
- tighten this into a **GitHub-style README**
- add **badges + quick-start**
- rewrite sections to sound more **academic / research-paper-adjacent**
- prep a **project website landing page version**
